{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62612ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from initialize_results_df import initialize_results_df\n",
    "from load_sequence_data import load_sequence_data\n",
    "from optuna_cnn_kmer_utils import *\n",
    "from k_mer_data_loader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382c5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-MER CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "stride = 1\n",
    "embedding_dim = 64\n",
    "max_len = 50  # Could do 97: (101 - K + 1) // S = 97\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab = build_kmer_vocab(k)\n",
    "vocab_size = len(vocab) + 1\n",
    "data_dir = '..\\\\Data'\n",
    "excel_dir = '..\\\\Outputs\\\\excel_results.xlsx'\n",
    "\n",
    "results_df, excel_df = initialize_results_df(data_dir, excel_dir)\n",
    "\n",
    "train_df = load_sequence_data(results_df['train_path'][1])\n",
    "test_df = load_sequence_data(results_df['test_path'][1])\n",
    "\n",
    "vocab = build_kmer_vocab(k)\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "train_loader, valid_loader, test_loader = prepare_kmer_loaders(\n",
    "    train_df['sequence'].tolist(), train_df['label'].values,\n",
    "    test_df['sequence'].tolist(), test_df['label'].values,\n",
    "    vocab, k, stride, max_len, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b8e132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 10:21:33,636] A new study created in memory with name: no-name-9c6968a9-8f11-4a66-b11c-a0ba0d56ae10\n",
      "[I 2025-05-02 10:21:39,682] Trial 0 finished with value: 0.5951492786407471 and parameters: {'num_layers': 4, 'embedding_dim': 64, 'units_0': 128, 'kernel_size_0': 11, 'activation_0': 'gelu', 'dropout_0': 0.3096533241889008, 'units_1': 128, 'kernel_size_1': 7, 'activation_1': 'silu', 'dropout_1': 0.396843340121502, 'units_2': 32, 'kernel_size_2': 5, 'activation_2': 'gelu', 'dropout_2': 0.3770255953635342, 'units_3': 128, 'kernel_size_3': 7, 'activation_3': 'gelu', 'dropout_3': 0.2490459672114775}. Best is trial 0 with value: 0.5951492786407471.\n",
      "[I 2025-05-02 10:21:47,042] Trial 1 finished with value: 0.611940324306488 and parameters: {'num_layers': 8, 'embedding_dim': 64, 'units_0': 128, 'kernel_size_0': 7, 'activation_0': 'gelu', 'dropout_0': 0.2517571691264483, 'units_1': 32, 'kernel_size_1': 7, 'activation_1': 'silu', 'dropout_1': 0.42124381299903235, 'units_2': 32, 'kernel_size_2': 11, 'activation_2': 'relu', 'dropout_2': 0.2151194903695739, 'units_3': 64, 'kernel_size_3': 7, 'activation_3': 'gelu', 'dropout_3': 0.4150442815358266, 'units_4': 128, 'kernel_size_4': 7, 'activation_4': 'silu', 'dropout_4': 0.3880892389484897, 'units_5': 128, 'kernel_size_5': 7, 'activation_5': 'relu', 'dropout_5': 0.13444223875666297, 'units_6': 64, 'kernel_size_6': 11, 'activation_6': 'silu', 'dropout_6': 0.38751685998232377, 'units_7': 32, 'kernel_size_7': 5, 'activation_7': 'relu', 'dropout_7': 0.14052723641779222}. Best is trial 1 with value: 0.611940324306488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 8, 'embedding_dim': 64, 'units_0': 128, 'kernel_size_0': 7, 'activation_0': 'gelu', 'dropout_0': 0.2517571691264483, 'units_1': 32, 'kernel_size_1': 7, 'activation_1': 'silu', 'dropout_1': 0.42124381299903235, 'units_2': 32, 'kernel_size_2': 11, 'activation_2': 'relu', 'dropout_2': 0.2151194903695739, 'units_3': 64, 'kernel_size_3': 7, 'activation_3': 'gelu', 'dropout_3': 0.4150442815358266, 'units_4': 128, 'kernel_size_4': 7, 'activation_4': 'silu', 'dropout_4': 0.3880892389484897, 'units_5': 128, 'kernel_size_5': 7, 'activation_5': 'relu', 'dropout_5': 0.13444223875666297, 'units_6': 64, 'kernel_size_6': 11, 'activation_6': 'silu', 'dropout_6': 0.38751685998232377, 'units_7': 32, 'kernel_size_7': 5, 'activation_7': 'relu', 'dropout_7': 0.14052723641779222}\n",
      "0.5858209133148193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_space = {\n",
    "    \"num_layers\": {\"type\": \"int\", \"low\": 4, \"high\": 8},\n",
    "    \"embedding_dim\": {\"type\": \"categorical\", \"choices\": [64]},\n",
    "    \"units\": {\"type\": \"categorical\", \"choices\": [32, 64, 128]},\n",
    "    \"kernel_size\": {\"type\": \"categorical\", \"choices\": [5, 7, 11]},\n",
    "    \"activation\": {\"type\": \"categorical\", \"choices\": [\"relu\", \"gelu\", \"silu\"]},\n",
    "    \"dropout\": {\"type\": \"float\", \"low\": 0.1, \"high\": 0.5},\n",
    "}\n",
    "\n",
    "best_model, best_params, acc, study = run_optuna_pipeline(\n",
    "    train_loader, valid_loader,\n",
    "    vocab_size=len(vocab)+1,\n",
    "    device='cuda',\n",
    "    epochs=10,\n",
    "    n_trials=2,\n",
    "    max_len=50,\n",
    "    save_path='best_model_kmer.pt',\n",
    "    search_space=search_space\n",
    ")\n",
    "\n",
    "print(best_params)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8efd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "\n",
    "study.best_params\n",
    "import json\n",
    "\n",
    "with open(\"final_model_hparams.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce83ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7341\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load the saved best model weights\n",
    "best_model.load_state_dict(torch.load('best_model_kmer.pt'))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "# ✅ Evaluate on test_loader\n",
    "acc_test, preds_test, labels_test = evaluate(best_model, test_loader, device)\n",
    "\n",
    "print(f\"Test Accuracy: {acc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004886a",
   "metadata": {},
   "source": [
    "# LOOPING THROUGH FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5da6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from initialize_results_df import initialize_results_df\n",
    "from load_sequence_data import load_sequence_data\n",
    "from optuna_cnn_kmer_utils import *\n",
    "from k_mer_data_loader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved at: ../Outputs/50_CNN_KM.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "data_dir = '../Data'\n",
    "excel_path = '../Outputs/50_CNN_KM.xlsx'\n",
    "\n",
    "# Load dataframes\n",
    "results_df, excel_df = initialize_results_df(data_dir, excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79d99440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hp from JSON\n",
    "with open('../Outputs/optuna_cnn_k_mers_hypparams.json', 'r') as f:\n",
    "    hp = json.load(f)\n",
    "    \n",
    "if 'embedding_dim' not in hp:\n",
    "    hp['embedding_dim'] = 64  # or whatever value you tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1ebae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DynamicCNN:\n\tMissing key(s) in state_dict: \"conv_layers.12.weight\", \"conv_layers.12.bias\". \n\tUnexpected key(s) in state_dict: \"conv_layers.11.weight\", \"conv_layers.11.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m max_len = \u001b[32m101\u001b[39m  \u001b[38;5;66;03m# or fixed, or largest length across all folders (up to you)\u001b[39;00m\n\u001b[32m      5\u001b[39m model = DynamicCNN(vocab_size, hp, max_len=max_len)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../Outputs/optuna_cnn_k_mers.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhair\\Desktop\\Main_Project\\capstone_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for DynamicCNN:\n\tMissing key(s) in state_dict: \"conv_layers.12.weight\", \"conv_layers.12.bias\". \n\tUnexpected key(s) in state_dict: \"conv_layers.11.weight\", \"conv_layers.11.bias\". "
     ]
    }
   ],
   "source": [
    "vocab = build_kmer_vocab(k=5)\n",
    "vocab_size = len(vocab) + 1  # +1 for padding\n",
    "max_len = 101  # or fixed, or largest length across all folders (up to you)\n",
    "\n",
    "model = DynamicCNN(vocab_size, hp, max_len=max_len)\n",
    "model.load_state_dict(torch.load('../Outputs/optuna_cnn_k_mers.pt'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model loaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d0f963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing wgEncodeAwgTfbsBroadDnd41CtcfUniPk\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# --- Fine-tune same model ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m optimizer = torch.optim.Adam(\u001b[43mmodel\u001b[49m.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     30\u001b[39m criterion = nn.BCEWithLogitsLoss()\n\u001b[32m     31\u001b[39m train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, row in results_df.iloc[:2].iterrows():\n",
    "    train_path = row['train_path']\n",
    "    test_path = row['test_path']\n",
    "    folder_name = row['folder_name']\n",
    "    \n",
    "    print(f\"🔄 Processing {folder_name}\")\n",
    "\n",
    "    # --- Load data ---\n",
    "    train_df = load_sequence_data(train_path)\n",
    "    test_df = load_sequence_data(test_path)\n",
    "\n",
    "    # --- Tokenize ---\n",
    "    X_train = [tokenize_sequence(seq, vocab, k=5, stride=2) for seq in train_df['sequence']]\n",
    "    X_test = [tokenize_sequence(seq, vocab, k=5, stride=2) for seq in test_df['sequence']]\n",
    "    y_train = train_df['label'].tolist()\n",
    "    y_test = test_df['label'].tolist()\n",
    "\n",
    "    # --- Compute max_len dynamically (or set fixed if preferred) ---\n",
    "    max_len = max(max(len(seq) for seq in X_train), max(len(seq) for seq in X_test))\n",
    "\n",
    "    # --- Prepare datasets/loaders ---\n",
    "    train_dataset = PreTokenizedDataset(X_train, y_train, max_len=max_len)\n",
    "    test_dataset = PreTokenizedDataset(X_test, y_test, max_len=max_len)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    # --- Fine-tune same model ---\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    train_acc, train_preds, train_labels = evaluate(model, train_loader, device)\n",
    "    test_acc, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    # Optional: calculate PR AUC, ROC AUC\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    train_probs = train_preds.numpy()\n",
    "    test_probs = test_preds.numpy()\n",
    "\n",
    "    train_pr_auc = average_precision_score(train_labels.numpy(), train_probs)\n",
    "    train_roc_auc = roc_auc_score(train_labels.numpy(), train_probs)\n",
    "\n",
    "    test_pr_auc = average_precision_score(test_labels.numpy(), test_probs)\n",
    "    test_roc_auc = roc_auc_score(test_labels.numpy(), test_probs)\n",
    "\n",
    "    # ✅ Log metrics\n",
    "    excel_df.at[idx, 'train_accuracy'] = train_acc\n",
    "    excel_df.at[idx, 'test_accuracy'] = test_acc\n",
    "    excel_df.at[idx, 'pr-roc'] = test_roc_auc\n",
    "    excel_df.at[idx, 'pr-auc'] = test_pr_auc\n",
    "\n",
    "    print(f\"✅ {folder_name}: train_acc={train_acc:.4f}, test_acc={test_acc:.4f}\")\n",
    "\n",
    "# # ✅ Save final model\n",
    "# torch.save(model.state_dict(), \"outputs/final_model.pt\")\n",
    "# print(\"✅ Final model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
